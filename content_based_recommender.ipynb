{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Content-Based Filtering for Movie Recommendations\n",
        "\n",
        "## 1 - Packages\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "\n",
        "## 2 - Movie ratings dataset\n",
        "# Load dataset (assume data files are in ./data/)\n",
        "top10_df = pd.read_csv(\"./data/content_top10_df.csv\")\n",
        "bygenre_df = pd.read_csv(\"./data/content_bygenre_df.csv\")\n",
        "\n",
        "## 3 - Content-based filtering with a neural network\n",
        "# Load data\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3\n",
        "num_item_features = item_train.shape[1] - 1\n",
        "uvs = 3\n",
        "ivs = 3\n",
        "u_s = 3\n",
        "i_s = 1\n",
        "\n",
        "# Scale data\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "\n",
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test = train_test_split(y_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "\n",
        "## 4 - Neural Network for content-based filtering\n",
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
        "])\n",
        "\n",
        "# Create model\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.MeanSquaredError())\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)\n",
        "\n",
        "## 5 - Predictions\n",
        "# Example new user prediction\n",
        "new_user_id = 5000\n",
        "new_rating_ave = 0.0\n",
        "# ... (fill in preferences as per lab)\n",
        "# Generate predictions...\n",
        "\n",
        "# Squared distance for similarity\n",
        "def sq_dist(a, b):\n",
        "    return np.sum((a - b) ** 2)\n",
        "\n",
        "# ... (further prediction and similarity code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from recsysNN_utils import load_data\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "tf.random.set_seed(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prepared matrices (uses MovieLens if found, else synthetic)\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3\n",
        "num_item_features = item_train.shape[1] - 1\n",
        "u_s = 3  # number of header cols in user_train\n",
        "i_s = 1  # number of header cols in item_train\n",
        "\n",
        "print(f\"item_train shape: {item_train.shape}\")\n",
        "print(f\"user_train shape: {user_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"num_user_features: {num_user_features}, num_item_features: {num_item_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features and split train/test\n",
        "item_scaler = StandardScaler()\n",
        "user_scaler = StandardScaler()\n",
        "\n",
        "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "item_scaled = item_scaler.fit_transform(item_train)\n",
        "user_scaled = user_scaler.fit_transform(user_train)\n",
        "y_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "X_item_tr, X_item_te = train_test_split(item_scaled, train_size=0.8, shuffle=True, random_state=1)\n",
        "X_user_tr, X_user_te = train_test_split(user_scaled, train_size=0.8, shuffle=True, random_state=1)\n",
        "Y_tr, Y_te = train_test_split(y_scaled, train_size=0.8, shuffle=True, random_state=1)\n",
        "\n",
        "print(X_item_tr.shape, X_user_tr.shape, Y_tr.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build two-tower content-based model\n",
        "num_outputs = 32\n",
        "\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(num_user_features,)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(num_item_features,)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
        "])\n",
        "\n",
        "input_user = tf.keras.layers.Input(shape=(user_NN.input_shape[1],))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "input_item = tf.keras.layers.Input(shape=(item_NN.input_shape[1],))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "history = model.fit([X_user_tr[:, u_s:], X_item_tr[:, i_s:]], Y_tr, epochs=15, batch_size=1024,\n",
        "                    validation_data=([X_user_te[:, u_s:], X_item_te[:, i_s:]], Y_te), verbose=1)\n",
        "\n",
        "print(\"Final val MSE:\", float(model.evaluate([X_user_te[:, u_s:], X_item_te[:, i_s:]], Y_te, verbose=0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference utilities\n",
        "\n",
        "def predict_user_item_scores(user_feature_row: np.ndarray, item_matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Predict scores for one user row vs many item rows (expects scaled inputs).\"\"\"\n",
        "    user_emb = user_NN(user_feature_row[None, :])\n",
        "    user_emb = tf.linalg.l2_normalize(user_emb, axis=1)\n",
        "\n",
        "    item_emb = item_NN(item_matrix)\n",
        "    item_emb = tf.linalg.l2_normalize(item_emb, axis=1)\n",
        "\n",
        "    scores = tf.linalg.matvec(item_emb, tf.squeeze(user_emb, axis=0))\n",
        "    return scores.numpy()\n",
        "\n",
        "\n",
        "def recommend_top_k_for_user(user_row_scaled: np.ndarray, item_scaled: np.ndarray, k: int = 10) -> np.ndarray:\n",
        "    scores = predict_user_item_scores(user_row_scaled[u_s:], item_scaled[:, i_s:])\n",
        "    top_k_idx = np.argsort(scores)[-k:][::-1]\n",
        "    return top_k_idx\n",
        "\n",
        "\n",
        "def squared_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sum((a - b) ** 2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: recommend for an existing user\n",
        "user_idx = 0\n",
        "user_row_scaled = X_user_te[user_idx]\n",
        "\n",
        "top_idx = recommend_top_k_for_user(user_row_scaled, X_item_te, k=10)\n",
        "movie_ids = item_train[:, 0].astype(int)[top_idx]  # map back via same order assumption\n",
        "\n",
        "titles = [movie_dict.get(\"id_to_title\", {}).get(int(mid), f\"Movie {int(mid)}\") for mid in movie_ids]\n",
        "list(zip(movie_ids, titles))[:10]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
